This project focuses on Human Action Recognition (HAR) using deep learning models that process video sequences to classify human activities. It leverages two architectures:
ConvLSTM: A model that combines convolutional operations with LSTM units for spatiotemporal learning.
LRCN (Long-term Recurrent Convolutional Network): A hybrid model that applies CNNs for spatial feature extraction and LSTMs for temporal sequence learning.

The UCF50 dataset, which contains 50 different human action categories, is used to train and test the models. 

The task is to predict the activity shown in a video clip.
